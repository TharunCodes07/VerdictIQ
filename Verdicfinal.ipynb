{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langchain_community pinecone-client python-dotenv tiktoken\n",
    "# pip install -U langchain-ollama\n",
    "# pip install pinecone\n",
    "# pip install llama-index pypdf\n",
    "# pip install einops accelerate sentence-transformers\n",
    "# pip install llama-index-llms-langchain\n",
    "# pip install chromadb\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv\n",
    "import csv\n",
    "file_name = \"D:\\Coding\\Python\\RAG\\VerdictIQ\\data\\COVID-Handbook-for-journalists.pdf\"\n",
    "def read_csv_file():\n",
    "    with open(file_name, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        return list(reader)\n",
    "def write_csv_file(data):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, page in enumerate(data):\n",
    "#             print(f\"Page {i + 1}:\")\n",
    "#             print(page.page_content)\n",
    "#             print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text):\n",
    "    clean_text = text.replace(\"\\n\", \" \").strip()\n",
    "    return clean_text\n",
    "\n",
    "def clean(data):\n",
    "    data_cleaned=[]\n",
    "    for i,page in enumerate(data):\n",
    "        data_cleaned.append({\n",
    "            \"Content\":text_formatter(page.page_content),\n",
    "            \"PageNumber\" : i+1\n",
    "        }\n",
    "        )\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting them into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "def sentencizer(pages_and_texts):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    for item in pages_and_texts:\n",
    "        item[\"sentences\"] = list(nlp(item[\"Content\"]).sents)\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list, slice_size):\n",
    "\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "def chunker(data,num_sentence_chunk_size):\n",
    "    for item in data:\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                            slice_size=num_sentence_chunk_size)\n",
    "        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def join_sentences(data):\n",
    "    pages_and_chunks = []\n",
    "    for item in data:\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"PageNumber\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "    return pages_and_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def embedding(final_chunked_data):\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\") \n",
    "    for item in final_chunked_data:\n",
    "        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting them into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_converter(final_chunked_data):\n",
    "    documents = [item[\"sentence_chunk\"] for item in final_chunked_data]\n",
    "    # pageNumbers = [item[\"page_number\"] for item in final_chunked_data]\n",
    "    embedding = [item[\"embedding\"].tolist() for item in final_chunked_data]\n",
    "\n",
    "    id = [f\"id{x}\" for x,item in enumerate(final_chunked_data)]\n",
    "    return documents, embedding,id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding them into a DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db(documents,embeddings,id,name):\n",
    "    import chromadb\n",
    "\n",
    "    chroma_client = chromadb.Client()\n",
    "\n",
    "    existing_collections = chroma_client.list_collections()\n",
    "    print(existing_collections)\n",
    "    if name in [col.name for col in existing_collections]:\n",
    "        collection = chroma_client.get_collection(name=name)\n",
    "        print(f\"Using existing collection: {name}\")\n",
    "    else:\n",
    "        collection = chroma_client.create_collection(name=name)\n",
    "        print(f\"Created new collection: {name}\")\n",
    "    collection.add(documents=documents,embeddings=embeddings,ids=id)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_result(query,collection,n_result):\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\") \n",
    "    query_embeddings = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embeddings, \n",
    "        n_results=n_result\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_engine(current_query,old_convo=None,context_summary=None):\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    llm = OllamaLLM(model=\"llama3.1\",temperature=0.2)\n",
    "    from langchain import PromptTemplate\n",
    "    old_convo_str = \" \".join([f\"Human: {prompt} | AI: {response}\" for prompt, response in old_convo]) if old_convo is not None else \"\"\n",
    "    if context_summary == None and old_convo == None:\n",
    "        template = \"create a query to a RAG model where the current prompt is {current_query}. Make sure the query only consists of key terms which the user wants to know about in the current prompt.\"\n",
    "    elif context_summary == None and old_convo != None:\n",
    "        template = \"create a query to a RAG model where the past three prompts along with responses are {old_convo_str} and the current prompt is {current_query}. Make sure the query only consists of key terms which the user wants to know about in the current prompt which may or may not be related to the previous conversations.\"\n",
    "    elif old_convo != None and context_summary != None:\n",
    "        template =\"create a query to a RAG model where the context of conversation so far is : {context_summary} and the past three promps along wiht response is {old_convo_str} and the current prompt is{current_query}. Make sure query only consists of key terms which the user wants to know about in the current prompt which may or may not be related to the previous conversations\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['current_query','old_convo_str','context_summary'],\n",
    "        template=template\n",
    "    )\n",
    "    response = llm(prompt.format(current_query = current_query,old_convo_str = old_convo_str,context_summary = context_summary))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatSummaryBufferChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ChatSummaryBufferChainTest:\n",
    "    def __init__(self, max_memory=2,summary_fn = None):\n",
    "        memory = read_csv_file()\n",
    "        print(memory)\n",
    "        self.max_memory = max_memory\n",
    "        self.summary_fn = summary_fn\n",
    "        x =()\n",
    "        if len(memory)>1:\n",
    "            x= tuple(memory[1])\n",
    "            self.memory = deque([x],maxlen=max_memory) \n",
    "        self.summaries = memory[0][0] if len(memory)>0 else \"\"\n",
    "\n",
    "\n",
    "    def add_conversation(self, human_message, ai_message):\n",
    "        self.memory.append((human_message, ai_message))\n",
    "        print(self.memory[-1])\n",
    "        if len(self.memory) == self.memory.maxlen and len(self.summaries) > 0:\n",
    "            print(\"length\",len(self.memory))\n",
    "            self.summarize_conversations()\n",
    "\n",
    "\n",
    "    def summarize_conversations(self):\n",
    "        if self.summary_fn and len(self.memory) > 0 and self.summaries==\"\":\n",
    "            if len(self.memory)==self.max_memory:\n",
    "                oldest_convo = self.memory.popleft()\n",
    "            summary = self.summary_fn(oldest_convo)\n",
    "            self.summaries = summary\n",
    "\n",
    "        elif self.summaries != \"\" and self.summary_fn and len(self.memory) > 0 :\n",
    "            oldest_convo = self.memory.popleft()\n",
    "            summary = self.summary_fn(oldest_convo,self.summaries)\n",
    "            self.summaries = summary\n",
    "        old_convo_memory = [(prompt, response) for prompt, response in self.memory]\n",
    "        print(\"old_convo_memory\",old_convo_memory)\n",
    "        write_csv_file([[self.summaries],old_convo_memory])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_fn(old_convo,summary_old=\"\"):\n",
    "    from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    from langchain import PromptTemplate\n",
    "    summarizer = OllamaLLM(model=\"llama3.1\")\n",
    "    template = ''' Your task is to combine the old summary with the latest conversation. \n",
    "        Focus on merging the key points from both the old summary and the new conversation into a single, concise summary that contains all relevant information.\n",
    "        Ensure that the final summary is brief but informative, covering all the essential details from both sources. Respond with the version which is a concice summary of the below two\n",
    "        \n",
    "        Old Summary: {old_summary}\n",
    "\n",
    "        New Conversation: {new_convo}\n",
    "\n",
    "        \n",
    "        '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['old_summary','new_convo'],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "You are an expert summarizer. Respond with only the summary text\n",
    "\"\"\"),\n",
    "    HumanMessage(prompt.format(old_summary=summary_old,new_convo=old_convo))\n",
    "]\n",
    "    summary = summarizer.invoke(messages)\n",
    "    print(\"Summary: \" , summary , \"\\n\\n\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(llm,collection):\n",
    "    buffer = ChatSummaryBufferChainTest(summary_fn=summarizer_fn)\n",
    "    query = input(\"Question:\")\n",
    "    while query != \"quit\":\n",
    "        context = search_result(query,collection,3)\n",
    "        context = context['documents']\n",
    "        context_str = ''\n",
    "        for i in context:\n",
    "            for j in i:\n",
    "                context_str += str(j)\n",
    "        old_convo = buffer.memory\n",
    "        print(\"Old convo: \", old_convo)\n",
    "        old_convo_str = \" \".join([f\"Human: {prompt} | AI: {response}\" for prompt, response in old_convo])\n",
    "        # query = query_engine(query,old_convo,buffer.summaries)\n",
    "        from langchain import PromptTemplate\n",
    "        template = ''' Answer the current question. You may use the information given in the context if necessary. You may also refer to the summary and the last two conversations that we has (old conversation)\n",
    "\n",
    "        Current Question: {query}\n",
    "\n",
    "        Context: {context_str}\n",
    "        \n",
    "        Summary : {summary}\n",
    "\n",
    "        old Conversation:{old_convo_str}\n",
    "\n",
    "        Respond with only the answer, do not add anything else\n",
    "        '''\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=['query','summary','old_covo_str','context'],\n",
    "            template=template\n",
    "        )\n",
    "        response = llm(prompt.format(query=query,summary=buffer.summaries,old_convo_str=old_convo_str,context_str = context_str))\n",
    "        buffer.add_conversation(query,response)\n",
    "        if  old_convo_str != \"\":\n",
    "            buffer.summarize_conversations()\n",
    "        print(\"Context: \", context_str + \"\\n\\n\")\n",
    "        print(\"Old convo: \", old_convo_str + \"\\n\\n\")\n",
    "\n",
    "        print(\"Response: \", response)\n",
    "        query=input(\"Question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document(\"/Users/sarveshwar/Documents/Semester-3/Projects/LLM/moon.txt\")\n",
    "final_data = clean(data)\n",
    "sentencizer(final_data)\n",
    "chunker(final_data,12)\n",
    "final_chunked_data = join_sentences(final_data)\n",
    "embedding(final_chunked_data)\n",
    "documents, embeddings,id = list_converter(final_chunked_data)\n",
    "collection = db(documents,embeddings,id,\"test2\")\n",
    "print(\"embeddings and collection creation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(model=\"llama3.1\",temperature=0.2)\n",
    "ask_with_memory(llm,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\",temperature=0.2)\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "template = ''' .\n",
    "\n",
    "Context: {context}'''\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['person','context'],\n",
    "    template=template\n",
    ")\n",
    "response = llm(prompt.format(person = \"JOSEPH B. MARTIN\",context = context[\"documents\"]))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaLLM\n",
    "# llm = OllamaLLM(model=\"llama3.1\",temperature=0.2)\n",
    "# print(llm.invoke(\"JOSEPH B. MARTIN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
